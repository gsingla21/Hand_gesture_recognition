{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Idf7mDIysG90es_LXen9aM0XlCouvbgb",
      "authorship_tag": "ABX9TyOsSy6ScKSZuNRjg6RVl6wD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gourish2/hand-gesture-recognition/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ShtwBQVHS9o",
        "outputId": "52c8403f-70c2-4da7-b830-6abb8d1f27ab"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import utils\n",
        "\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.layers import Dense, Input, Dropout,Flatten, Conv2D\n",
        "\n",
        "from keras.layers import BatchNormalization, Activation, MaxPooling2D\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from IPython.display import SVG, Image\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"Tensorflow version:\", tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version: 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmlPeZ6qHh4A",
        "outputId": "65bf4f25-e02e-4807-c47d-2fc50133b300"
      },
      "source": [
        "for expression in os.listdir(\"/content/drive/MyDrive/train\"):\n",
        "  print(str(len(os.listdir(\"/content/drive/MyDrive/train/\"+expression)))+\" \"+expression+' images')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200 five images\n",
            "200 four images\n",
            "200 three images\n",
            "200 one images\n",
            "200 two images\n",
            "200 zero images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYmzB1qQIKPF",
        "outputId": "639875b2-f759-4a25-b8ac-817301a13d8b"
      },
      "source": [
        "img_size=64\n",
        "\n",
        "batch_size=64\n",
        "\n",
        "datagen_train=ImageDataGenerator(horizontal_flip=True)\n",
        "\n",
        "train_generator=datagen_train.flow_from_directory(\"/content/drive/MyDrive/train\",\n",
        "\n",
        "                                                 target_size=(img_size,img_size),\n",
        "\n",
        "                                                 color_mode='grayscale',\n",
        "\n",
        "                                                 batch_size=batch_size,\n",
        "\n",
        "                                                 class_mode='categorical',\n",
        "\n",
        "                                                 shuffle=True)\n",
        "\n",
        "datagen_validation=ImageDataGenerator(horizontal_flip=True)\n",
        "\n",
        "validation_generator=datagen_train.flow_from_directory(\"/content/drive/MyDrive/train\",\n",
        "\n",
        "                                                 target_size=(img_size,img_size),\n",
        "\n",
        "                                                 color_mode='grayscale',\n",
        "\n",
        "                                                 batch_size=batch_size,\n",
        "\n",
        "                                                 class_mode='categorical',\n",
        "\n",
        "                                                 shuffle=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1200 images belonging to 6 classes.\n",
            "Found 1200 images belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZs3Q9caIXA3",
        "outputId": "f0355bdd-7cd3-4cae-9353-45de7c36487d"
      },
      "source": [
        "model=Sequential()\n",
        "\n",
        "#conv-1\n",
        "\n",
        "model.add(Conv2D(64,(3,3),padding='same',input_shape=(64,64,1)))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "#2 -conv layer\n",
        "\n",
        "model.add(Conv2D(128,(5,5),padding='same'))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "#3 -conv layer\n",
        "\n",
        "model.add(Conv2D(512,(3,3),padding='same'))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "#4 -conv layer\n",
        "\n",
        "model.add(Conv2D(512,(3,3),padding='same'))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(512))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(6,activation='softmax'))\n",
        "\n",
        "opt=Adam(lr=0.0005)\n",
        "\n",
        "#lr-learning rate\n",
        "\n",
        "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 64, 64, 64)        640       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 512)       590336    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 16, 512)       2048      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 8, 8, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               2097408   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 3078      \n",
            "=================================================================\n",
            "Total params: 5,395,718\n",
            "Trainable params: 5,391,750\n",
            "Non-trainable params: 3,968\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z61a8HYIejn",
        "outputId": "39d19163-5ae9-4419-a617-ff2a2b9f36dc"
      },
      "source": [
        "ephocs=15\n",
        "\n",
        "steps_per_epoch=train_generator.n//train_generator.batch_size\n",
        "\n",
        "steps_per_epoch\n",
        "\n",
        "validation_steps=validation_generator.n//validation_generator.batch_size\n",
        "\n",
        "validation_steps\n",
        "\n",
        "history=model.fit(\n",
        "\n",
        "    x=train_generator,\n",
        "\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "\n",
        "    epochs=ephocs,\n",
        "\n",
        "    validation_data=validation_generator,\n",
        "\n",
        "    validation_steps=validation_steps,\n",
        "\n",
        " #   callbacks=callbacks\n",
        "\n",
        ")\n",
        "\n",
        "model.save('hand_gesture.h5')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "18/18 [==============================] - 393s 22s/step - loss: 1.0101 - accuracy: 0.6100 - val_loss: 153.0934 - val_accuracy: 0.1649\n",
            "Epoch 2/15\n",
            "18/18 [==============================] - 105s 6s/step - loss: 0.3318 - accuracy: 0.8935 - val_loss: 127.4513 - val_accuracy: 0.1649\n",
            "Epoch 3/15\n",
            "18/18 [==============================] - 102s 6s/step - loss: 0.1645 - accuracy: 0.9542 - val_loss: 84.8090 - val_accuracy: 0.1658\n",
            "Epoch 4/15\n",
            "18/18 [==============================] - 102s 6s/step - loss: 0.0933 - accuracy: 0.9798 - val_loss: 48.5075 - val_accuracy: 0.1701\n",
            "Epoch 5/15\n",
            "18/18 [==============================] - 104s 6s/step - loss: 0.0764 - accuracy: 0.9806 - val_loss: 31.0787 - val_accuracy: 0.1658\n",
            "Epoch 6/15\n",
            "18/18 [==============================] - 102s 6s/step - loss: 0.0513 - accuracy: 0.9886 - val_loss: 20.5443 - val_accuracy: 0.1615\n",
            "Epoch 7/15\n",
            "18/18 [==============================] - 102s 6s/step - loss: 0.0505 - accuracy: 0.9903 - val_loss: 9.6856 - val_accuracy: 0.2153\n",
            "Epoch 8/15\n",
            "18/18 [==============================] - 105s 6s/step - loss: 0.0434 - accuracy: 0.9894 - val_loss: 3.1820 - val_accuracy: 0.3229\n",
            "Epoch 9/15\n",
            "18/18 [==============================] - 104s 6s/step - loss: 0.0385 - accuracy: 0.9887 - val_loss: 0.4276 - val_accuracy: 0.8446\n",
            "Epoch 10/15\n",
            "18/18 [==============================] - 103s 6s/step - loss: 0.0233 - accuracy: 0.9965 - val_loss: 0.1923 - val_accuracy: 0.9132\n",
            "Epoch 11/15\n",
            "18/18 [==============================] - 108s 6s/step - loss: 0.0254 - accuracy: 0.9921 - val_loss: 0.1022 - val_accuracy: 0.9583\n",
            "Epoch 12/15\n",
            "18/18 [==============================] - 104s 6s/step - loss: 0.0284 - accuracy: 0.9938 - val_loss: 0.1027 - val_accuracy: 0.9549\n",
            "Epoch 13/15\n",
            "18/18 [==============================] - 103s 6s/step - loss: 0.0415 - accuracy: 0.9894 - val_loss: 0.1620 - val_accuracy: 0.9332\n",
            "Epoch 14/15\n",
            "18/18 [==============================] - 108s 6s/step - loss: 0.0197 - accuracy: 0.9947 - val_loss: 0.1921 - val_accuracy: 0.9149\n",
            "Epoch 15/15\n",
            "18/18 [==============================] - 103s 6s/step - loss: 0.0150 - accuracy: 0.9947 - val_loss: 0.4668 - val_accuracy: 0.8099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "8PmqccAsQjcz",
        "outputId": "eaa75b6e-60b0-4382-9d84-93379241e7ee"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing import image\n",
        "\n",
        "test_image = image.load_img('/content/drive/MyDrive/train/one/one101.png', target_size = (64,64),color_mode = \"grayscale\")\n",
        "\n",
        "plt.imshow(test_image)\n",
        "\n",
        "test_image = image.img_to_array(test_image)\n",
        "\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "\n",
        "result = model.predict(test_image)\n",
        "\n",
        "a=result.argmax()\n",
        "\n",
        "s=train_generator.class_indices\n",
        "\n",
        "        #print(s)\n",
        "\n",
        "name=[]\n",
        "\n",
        "for i in s:\n",
        "\n",
        "    name.append(i)\n",
        "\n",
        "for i in range(len(s)):\n",
        "\n",
        "    if(i==a):\n",
        "\n",
        "        q=name[i]\n",
        "\n",
        "print(q)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "one\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU4MvxYWRUz9"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model=load_model('/content/hand_gesture.h5')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "Exgq9dVARhwZ",
        "outputId": "deb0b637-b143-49b6-cefc-c69855acce51"
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import cv2\n",
        "\n",
        "#we are starting our web cam\n",
        "\n",
        "webcam=cv2.VideoCapture(0)\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# Category dictionary\n",
        "\n",
        "categories = {0: 'ZERO', 1: 'ONE', 2: 'TWO', 3: 'THREE', 4: 'FOUR', 5: 'FIVE'}\n",
        "\n",
        "s=\"\"\n",
        "\n",
        "d={}\n",
        "\n",
        "p=\"\"\n",
        "\n",
        "count=0\n",
        "\n",
        "while True:\n",
        "  _, frame = cap.read()\n",
        "  # Simulating mirror image\n",
        "  frame = cv2.flip(frame, 1)\n",
        "  # Got this from collect-data.py\n",
        "  # Coordinates of the ROIimport sys\n",
        "\n",
        "  x1 = int(0.5*frame.shape[1])\n",
        "  y1 = 10\n",
        "  x2 = frame.shape[1]-10\n",
        "  y2 = int(0.5*frame.shape[1])\n",
        "  # Drawing the ROI\n",
        "  # The increment/decrement by 1 is to compensate for the bounding box\n",
        "  cv2.rectangle(frame, (x1-1, y1-1), (x2+1, y2+1), (255,0,0) ,1)\n",
        "  # Extracting the ROI\n",
        "  roi = frame[y1:y2, x1:x2]\n",
        "  # Resizing the ROI so it can be fed to the model for prediction\n",
        "  roi = cv2.resize(roi, (64, 64)) \n",
        "  roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
        "  _, test_image = cv2.threshold(roi, 120, 255, cv2.THRESH_BINARY)\n",
        "  cv2.imshow(\"test\", test_image)\n",
        "  # Batch of 1\n",
        "  result = loaded_model.predict(test_image.reshape(1, 64, 64, 1))\n",
        "  prediction = {'FIVE': result[0][0], \n",
        "                'FOUR': result[0][1], \n",
        "                'ONE': result[0][2],\n",
        "                'THREE': result[0][3],\n",
        "                'TWO': result[0][4],\n",
        "                'ZERO': result[0][5]}\n",
        "  max_key = max(prediction, key=prediction.get)\n",
        "  cv2.putText(test_image,max_key,(x1,y1),cv2.FONT_HERSHEY_SIMPLEX,1,(255, 0, 0),2) \n",
        "  print(max_key)\n",
        "  cv2.imshow(\"Frame\", frame)\n",
        "  interrupt = cv2.waitKey(2)\n",
        "  if interrupt & 0xFF == 27: \n",
        "    # esc key\n",
        "    break\n",
        "\n",
        "cap.release()\n",
        "\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-fdea05592fcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;31m# Got this from collect-data.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;31m# Coordinates of the ROI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    }
  ]
}